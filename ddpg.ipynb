{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "# get gpu info\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from IPython.core.display_functions import display\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "# import cv2 as cv\n",
    "from collections import deque\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import os\n",
    "# from einops import rearrange, reduce, repeat\n",
    "# from einops.layers.torch import Rearrange, Reduce\n",
    "# from torchsummary import summary\n",
    "# from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "def generate_cropped_im(im, cropped_dims):\n",
    "    h_shift = 10\n",
    "    s_shift = 10\n",
    "    v_shift = 51\n",
    "    # generate random crop shifts\n",
    "    crop_attributes = {}\n",
    "    cropped_shift = [np.random.randint(0,im.size[0]-cropped_dims[0]),np.random.randint(0,im.size[1]-cropped_dims[1])]\n",
    "    # cropped_shift = [37,37]\n",
    "    crop_coords = (cropped_shift[0], cropped_shift[1], cropped_shift[0]+cropped_dims[0], cropped_shift[1]+cropped_dims[1])\n",
    "    cropped_im = im.crop(crop_coords)\n",
    "    cropped_im = cropped_im.convert('HSV')\n",
    "    h, s, v = cropped_im.split()\n",
    "    h = h.point(lambda p: p + h_shift)\n",
    "    s = s.point(lambda p: p + s_shift)\n",
    "    v = v.point(lambda p: p + v_shift)\n",
    "    cropped_im = Image.merge('HSV', (h, s, v))\n",
    "    cropped_im = cropped_im.convert('RGB')\n",
    "    cropped_im_shape = cropped_im.size\n",
    "    crop_attributes['crop_coords'] = crop_coords\n",
    "    crop_attributes['cropped_shift'] = cropped_shift\n",
    "    crop_attributes['cropped_im'] = cropped_im\n",
    "    crop_attributes['cropped_im_shape'] = cropped_im_shape\n",
    "    \n",
    "    return crop_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "class Env:\n",
    "    def __init__(self, fixed_image, moving_image, crop_coords=(37, 37, 74, 74)):\n",
    "        self.crop_coords = crop_coords\n",
    "        self.fixed_image = fixed_image\n",
    "        self.moving_image = moving_image\n",
    "        self.fixed_x = fixed_image.size[0]\n",
    "        self.fixed_y = fixed_image.size[1]\n",
    "        self.crop_x = crop_coords[0]\n",
    "        self.crop_y = crop_coords[1]\n",
    "        self.shift_x = 0\n",
    "        self.shift_y = 0\n",
    "        self.x_limit = fixed_image.size[0] - moving_image.size[0]\n",
    "        self.y_limit = fixed_image.size[1] - moving_image.size[1]\n",
    "        self.env_image = fixed_image.copy()\n",
    "        self.env_image.paste(moving_image, (0,0))\n",
    "        self.action_space = 4\n",
    "\n",
    "#     def reset(self):\n",
    "#         # move image to a random position\n",
    "#         print(self.x_limit)\n",
    "#         self.shift_x = np.random.randint(0, self.x_limit)\n",
    "#         self.shift_y = np.random.randint(0, self.y_limit)\n",
    "#         self.env_image = self.fixed_image.copy()\n",
    "#         self.env_image.paste(self.moving_image, (self.shift_x, self.shift_y))\n",
    "#         return self.env_image\n",
    "    \n",
    "    def reset(self):\n",
    "        crop_attr = generate_cropped_im(self.fixed_image, [37,37])\n",
    "        self.crop_coords = crop_attr['crop_coords']\n",
    "        print(f\"Crop coords: {self.crop_coords}\")\n",
    "        self.moving_image = crop_attr['cropped_im']\n",
    "        self.crop_x = self.crop_coords[0]\n",
    "        self.crop_y = self.crop_coords[1]\n",
    "        self.x_limit = self.fixed_image.size[0] - self.moving_image.size[0]\n",
    "        self.y_limit = self.fixed_image.size[1] - self.moving_image.size[1]\n",
    "        # move image to a random position\n",
    "        print(self.x_limit)\n",
    "        self.shift_x = np.random.randint(0, self.x_limit)\n",
    "        self.shift_y = np.random.randint(0, self.y_limit)\n",
    "        self.env_image = self.fixed_image.copy()\n",
    "        self.env_image.paste(self.moving_image, (self.shift_x, self.shift_y))\n",
    "        \n",
    "        return self.env_image\n",
    "\n",
    "    def check_frame(self, _im):\n",
    "        _im_shape = _im.size\n",
    "        _im_shape = [_im_shape[0] - 1, _im_shape[1] - 1]\n",
    "        # check bottom border\n",
    "        for i in range(_im_shape[0]):\n",
    "            px = _im.getpixel((i, 0))\n",
    "            if not (px[0] == 0 and px[1] == 0 and px[2] == 0):\n",
    "                return False\n",
    "        # check top border\n",
    "        for i in range(_im_shape[0]):\n",
    "            px = _im.getpixel((i, _im_shape[1]))\n",
    "            if not (px[0] == 0 and px[1] == 0 and px[2] == 0):\n",
    "                return False\n",
    "        # check left border\n",
    "        for i in range(_im_shape[1]):\n",
    "            px = _im.getpixel((0, i))\n",
    "            if not (px[0] == 0 and px[1] == 0 and px[2] == 0):\n",
    "                return False\n",
    "        # check right border\n",
    "        for i in range(_im_shape[1]):\n",
    "            px = _im.getpixel((_im_shape[0], i))\n",
    "            if not (px[0] == 0 and px[1] == 0 and px[2] == 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def move_image(self, _x, _y):\n",
    "        EDGE_PENALTY = 0\n",
    "        shift_x = int(self.shift_x + _x)\n",
    "        shift_y = int(self.shift_y + _y)\n",
    "        # shift_x = _x\n",
    "        # shift_y = _y\n",
    "        # check if image can be moved\n",
    "        reward_bonus = 0\n",
    "        if shift_x > self.x_limit:\n",
    "            shift_x = self.x_limit\n",
    "            reward_bonus -= EDGE_PENALTY\n",
    "            # print('x limit reached')\n",
    "        elif shift_x < 0:\n",
    "            shift_x = 0\n",
    "            reward_bonus -= EDGE_PENALTY\n",
    "            # print('x limit reached')\n",
    "        if shift_y > self.y_limit:\n",
    "            shift_y = self.y_limit\n",
    "            reward_bonus -= EDGE_PENALTY\n",
    "            # print('y limit reached')\n",
    "        elif shift_y < 0:\n",
    "            shift_y = 0\n",
    "            reward_bonus -= EDGE_PENALTY\n",
    "            # print('y limit reached')\n",
    "        if shift_x < 0 or shift_y < 0:\n",
    "            print(f\"Image can't be moved to {shift_x}, {shift_y}\")\n",
    "            return self.env_image, self.get_reward()\n",
    "        self.shift_x = shift_x\n",
    "        self.shift_y = shift_y\n",
    "        env_copy = self.fixed_image.copy()\n",
    "        env_copy.paste(self.moving_image, (shift_x, shift_y))\n",
    "        self.env_image = env_copy\n",
    "        reward = self.get_reward()\n",
    "        # print(f\"shift x {shift_x}, shift y {shift_y}, moved x {_x}, moved y {_y} moved x {self.crop_x}, moved y {self.crop_y}, reward {reward}\")\n",
    "        return env_copy, reward, reward_bonus\n",
    "\n",
    "    def get_reward(self):\n",
    "        distance = np.sqrt((self.shift_x - self.crop_x)**2 + (self.shift_y - self.crop_y)**2)\n",
    "        return -distance\n",
    "\n",
    "    def get_target(self, shift_x, shift_y):\n",
    "        _x = self.crop_x - shift_x\n",
    "        _y = self.crop_y - shift_y\n",
    "        target_x = 0.5+(_x/(2*self.x_limit))\n",
    "        target_y = 0.5+(_y/(2*self.y_limit))\n",
    "        return target_x, target_y\n",
    "\n",
    "    def get_pred_target(self, pred):\n",
    "        shift_x = (pred[0]*2-1) * self.x_limit\n",
    "        shift_y = (pred[1]*2-1) * self.y_limit\n",
    "        _x = self.crop_x - shift_x\n",
    "        _y = self.crop_y - shift_y\n",
    "        pred_target_x = 0.5+(_x/(2*self.x_limit))\n",
    "        pred_target_y = 0.5+(_y/(2*self.y_limit))\n",
    "        return pred_target_x, pred_target_y\n",
    "        # return _x, _y\n",
    "\n",
    "    def move_image_old(self, x, y):\n",
    "        # move image using pil\n",
    "        new_im = self.moving_image.transform(self.moving_image.size, Image.AFFINE, (1, 0, x, 0, 1, y))\n",
    "        can_move = self.check_frame(new_im)\n",
    "        if can_move:\n",
    "            self.moving_image = new_im\n",
    "            return new_im\n",
    "        else:\n",
    "            return self.moving_image\n",
    "\n",
    "    def step(self, action, amount=1):\n",
    "        \"\"\"\n",
    "        action: 0 = left, 1 = right, 2 = up, 3 = down\n",
    "        \"\"\"\n",
    "        \n",
    "        x_amount = action[0][0]*self.fixed_image.size[0]\n",
    "        y_amount = action[0][1]*self.fixed_image.size[1]\n",
    "        new_im, reward, bonus = self.move_image(x_amount, y_amount)\n",
    "\n",
    "        if reward >= -1:\n",
    "            reward = (self.fixed_image.size[0] + 100)\n",
    "            return new_im, reward+bonus, True\n",
    "        else:\n",
    "            return new_im, (reward+bonus), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "def preprocess_data(path, display=True):\n",
    "    h_shift = 0\n",
    "    s_shift = 0\n",
    "    v_shift = 0\n",
    "    envs = []\n",
    "    for i, filename in enumerate(os.listdir(path)):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            if i == 100:\n",
    "                break\n",
    "            im = Image.open('/'.join([path, filename]))\n",
    "            im_shape = im.size\n",
    "            if im_shape[0] >= 112 and im_shape[1] >= 112:\n",
    "                im_scale_x = im_shape[0]/112\n",
    "                im_scale_y = im_shape[1]/112\n",
    "\n",
    "                scale = 1/2\n",
    "                im = im.resize((int(im_shape[0]/im_scale_x),int(im_shape[1]/im_scale_y)),Image.ANTIALIAS)\n",
    "                im_shape = im.size\n",
    "                im = im.crop((0, 0, 112, 112))\n",
    "                im_shape = im.size\n",
    "                cropped_dims = [37, 37]\n",
    "                crop_attr = generate_cropped_im(im, cropped_dims)\n",
    "                cropped_im = crop_attr['cropped_im']\n",
    "                # change cropped image hue\n",
    "                cropped_im = cropped_im.convert('HSV')\n",
    "                cropped_im_shape = crop_attr['cropped_im_shape']\n",
    "                print(f'im_shape: {im_shape}, cropped_im_shape: {cropped_im_shape}')\n",
    "                crop_coords = crop_attr['crop_coords']\n",
    "\n",
    "                # red_dot = Image.new('RGB', (10, 10), color = 'red')\n",
    "                # green_dot = Image.new('RGB', (10, 10), color = 'green')\n",
    "                env_im = im\n",
    "                # cropped_im.paste(red_dot, (0,0))\n",
    "                fixed_x = crop_coords[0]\n",
    "                fixed_y = crop_coords[1]\n",
    "                shift_x = im_shape[0] - cropped_im_shape[0]\n",
    "                shift_y = 0\n",
    "\n",
    "                fixed = (fixed_x, fixed_y)\n",
    "                moving = (shift_x, shift_y)\n",
    "\n",
    "                distance = np.sqrt((fixed[0] - moving[0])**2 + (fixed[1] - moving[1])**2)\n",
    "\n",
    "                # env_im.paste(cropped_im, (shift_x, shift_y))\n",
    "                # env_im.paste(green_dot, (fixed_x, fixed_y))\n",
    "                env = Env(im, cropped_im, crop_coords)\n",
    "                envs.append(env)\n",
    "                print(f'image: {path},  distance: {distance}')\n",
    "                im2 = im.copy()\n",
    "                if display:\n",
    "                    plt.imshow(im2)\n",
    "                    plt.show()\n",
    "                    plt.imshow(cropped_im)\n",
    "                    plt.show()\n",
    "                # resize to imagenet size\n",
    "                transform = Compose([Resize((112, 112)), ToTensor()])\n",
    "                x = transform(im)\n",
    "                x = x.unsqueeze(0) # add batch dim\n",
    "                x.shape\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "envs = preprocess_data('coffee', display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cuda:1')\n",
    "print(device)\n",
    "\n",
    "env = envs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "im_b = env.reset()\n",
    "reward = env.get_reward()\n",
    "print(f\"Reward: {reward}\")\n",
    "plt.imshow(im_b)\n",
    "plt.show()\n",
    "# print(f\"shift coords: {env.shift_x}, {env.shift_y}\")\n",
    "# print(f\"moving coords: {env.crop_x}, {env.crop_y}\")\n",
    "# print(f\"diff: {env.crop_x-env.shift_x}, {env.crop_y-env.shift_y}\")\n",
    "targets = env.get_pred_target([0.5+(env.shift_x/(2*env.x_limit)),0.5+(env.shift_y/(2*env.y_limit))])\n",
    "print(env.crop_x-env.shift_x+0 ,env.crop_y - env.shift_y+0)\n",
    "im_a, r, b = env.move_image(env.crop_x-env.shift_x ,env.crop_y - env.shift_y)\n",
    "print(r)\n",
    "plt.imshow(im_a)\n",
    "plt.title(f'reward: {r}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "def plotLearning(scores, filename, x=None, window=5):   \n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "    if x is None:\n",
    "        x = [i for i in range(N)]\n",
    "    plt.ylabel('Score')       \n",
    "    plt.xlabel('Game')                     \n",
    "    plt.plot(x, running_avg)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddpg_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# resize = Compose([torch.ToPILImage(),\n",
    "#                     torch.Resize(112, interpolation=Image.CUBIC),\n",
    "#                     torch.ToTensor()])\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(112, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\n",
    "                                                            self.mu, self.sigma)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.con_bn1 = nn.GroupNorm(16,16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.con_bn2 = nn.GroupNorm(32,32)\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(112))\n",
    "        convh = conv2d_size_out(conv2d_size_out(112))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.fc1 = nn.Linear(linear_input_size, self.fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        torch.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        torch.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        #self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        #self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        #f2 = 0.002\n",
    "        torch.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        torch.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        #self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        #self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
    "        f3 = 0.003\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        torch.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        torch.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "        #self.q.weight.data.uniform_(-f3, f3)\n",
    "        #self.q.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = F.relu(self.con_bn1(self.conv1(state)))\n",
    "        state_value = self.conv2(state_value)\n",
    "        state_value = self.con_bn2(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc1(state_value.view(state_value.size(0), -1))\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = F.relu(self.action_value(action))\n",
    "        state_action_value = F.relu(torch.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = '/'.join([chkpt_dir,name+'_ddpg'])\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.con_bn1 = nn.GroupNorm(16,16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.con_bn2 = nn.GroupNorm(32,32)\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(112))\n",
    "        convh = conv2d_size_out(conv2d_size_out(112))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.fc1 = nn.Linear(linear_input_size, self.fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        torch.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        torch.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        #self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        #self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        #f2 = 0.002\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        torch.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        torch.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        #self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        #self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        #f3 = 0.004\n",
    "        f3 = 0.003\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        torch.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        torch.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "        #self.mu.weight.data.uniform_(-f3, f3)\n",
    "        #self.mu.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv1(state)\n",
    "        x = self.con_bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.con_bn2(self.conv2(x)))\n",
    "        x = self.fc1(x.view(x.size(0), -1))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.tanh(self.mu(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "def transform_obs(observation):\n",
    "        observation = np.ascontiguousarray(observation, dtype=np.float32) / 255\n",
    "        observation = torch.from_numpy(observation)\n",
    "        observation = resize(observation.T)\n",
    "        observation = observation.unsqueeze(0)\n",
    "        return observation\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, input_dims, tau, gamma=0.99,\n",
    "                 n_actions=2, max_size=10000, layer1_size=400,\n",
    "                 layer2_size=300, batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                  layer2_size, n_actions=n_actions,\n",
    "                                  name='Actor')\n",
    "        self.critic = CriticNetwork(beta, input_dims, layer1_size,\n",
    "                                    layer2_size, n_actions=n_actions,\n",
    "                                    name='Critic')\n",
    "\n",
    "        self.target_actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                         layer2_size, n_actions=n_actions,\n",
    "                                         name='TargetActor')\n",
    "        self.target_critic = CriticNetwork(beta, input_dims, layer1_size,\n",
    "                                           layer2_size, n_actions=n_actions,\n",
    "                                           name='TargetCritic')\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = transform_obs(observation)\n",
    "        \n",
    "        # observation = torch.tensor(observation, dtype=torch.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation.to(self.actor.device)).to(self.actor.device)\n",
    "        mu_prime = mu + torch.tensor(self.noise(),\n",
    "                                 dtype=torch.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                      self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        reward = torch.tensor(reward, dtype=torch.float).to(self.critic.device)\n",
    "        done = torch.tensor(done).to(self.critic.device)\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float).to(self.critic.device)\n",
    "        action = torch.tensor(action, dtype=torch.float).to(self.critic.device)\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_ = self.target_critic.forward(new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.batch_size):\n",
    "            target.append(reward[j] + self.gamma*critic_value_[j]*done[j])\n",
    "        target = torch.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.batch_size, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = torch.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        #Verify that the copy assignment worked correctly\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(target_critic_params)\n",
    "        actor_state_dict = dict(target_actor_params)\n",
    "        print('\\nActor Networks', tau)\n",
    "        for name, param in self.actor.named_parameters():\n",
    "            print(name, torch.equal(param, actor_state_dict[name]))\n",
    "        print('\\nCritic Networks', tau)\n",
    "        for name, param in self.critic.named_parameters():\n",
    "            print(name, torch.equal(param, critic_state_dict[name]))\n",
    "        input()\n",
    "        \"\"\"\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "    def check_actor_params(self):\n",
    "        current_actor_params = self.actor.named_parameters()\n",
    "        current_actor_dict = dict(current_actor_params)\n",
    "        original_actor_dict = dict(self.original_actor.named_parameters())\n",
    "        original_critic_dict = dict(self.original_critic.named_parameters())\n",
    "        current_critic_params = self.critic.named_parameters()\n",
    "        current_critic_dict = dict(current_critic_params)\n",
    "        print('Checking Actor parameters')\n",
    "\n",
    "        for param in current_actor_dict:\n",
    "            print(param, torch.equal(original_actor_dict[param], current_actor_dict[param]))\n",
    "        print('Checking critic parameters')\n",
    "        for param in current_critic_dict:\n",
    "            print(param, torch.equal(original_critic_dict[param], current_critic_dict[param]))\n",
    "        input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "from fileinput import filename\n",
    "import json\n",
    "\n",
    "def save_episode_data_json(episode, durations, scores):\n",
    "    data = {\"episode\": episode, \"durations\": durations, \"scores\": scores}\n",
    "    filename = \"tmp/episode_data_dict.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_episode_data_json():\n",
    "    filename = \"tmp/episode_data_dict.json\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "# from ddpg_torch import Agent\n",
    "# import gym\n",
    "import random\n",
    "import numpy as np\n",
    "# from utils import plotLearning\n",
    "\n",
    "EPSILON = 0.95\n",
    "EPSILON_DECAY = 0.99975\n",
    "EPSILON_MIN = 0.15\n",
    "\n",
    "# env = gym.make('LunarLanderContinuous-v2')\n",
    "env = envs[0]\n",
    "agent = Agent(alpha=0.000025, beta=0.00025, max_size=100000, input_dims=[3,112,112], tau=0.001,\n",
    "              batch_size=64,  layer1_size=400, layer2_size=300, n_actions=2)\n",
    "\n",
    "agent.load_models()\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "score_history = []\n",
    "episode_history = []\n",
    "episode_data = load_episode_data_json()\n",
    "episode = episode_data[\"episode\"] +1\n",
    "score_history = episode_data[\"scores\"]\n",
    "episode_history = episode_data[\"durations\"]\n",
    "\n",
    "for i in range(episode, 1000):\n",
    "    EPSILON = 0.95\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    env = random.choice(envs)\n",
    "    t = 0\n",
    "    while not done:\n",
    "        t += 1\n",
    "        # choose action by epsilon greedy\n",
    "        if np.random.random() > EPSILON:\n",
    "            act = agent.choose_action(obs)\n",
    "        else:\n",
    "            act = np.array([np.array([random.uniform(-1,1), random.uniform(-1,1)])])\n",
    "\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON *= EPSILON_DECAY\n",
    "\n",
    "        # act = agent.choose_action(obs)\n",
    "        new_state, reward, done = env.step(act)\n",
    "        agent.remember(transform_obs(obs), act, reward, transform_obs(new_state), int(done))\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "        if t%100 == 0:\n",
    "            x_amount = act[0][0]*env.fixed_image.size[0]\n",
    "            y_amount = act[0][1]*env.fixed_image.size[1]\n",
    "            print(f'Episode: {i}.{t} Score: {score.round(2)}, reward: {reward} x: {x_amount.round(2)}, y: {y_amount.round(2)} epsilon: {EPSILON}')\n",
    "            # plot the state\n",
    "        if t%2000 == 0:\n",
    "            # plot the state\n",
    "            env = random.choice(envs)\n",
    "            EPSILON = 0.95\n",
    "            # plt.imshow(obs)\n",
    "            # plt.show()\n",
    "            \n",
    "        #env.render()\n",
    "    score_history.append(score)\n",
    "    episode_history.append(t)\n",
    "    plt.imshow(obs)\n",
    "    plt.title(f'Done, Episode: {i} Score: {score.round(2)}')\n",
    "    plt.show()\n",
    "\n",
    "    # plot and save the score\n",
    "    plt.plot(episode_history)\n",
    "    plt.savefig('runtime_episodes.png')\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "       agent.save_models()\n",
    "       save_episode_data_json(i, episode_history, score_history)\n",
    "\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))\n",
    "\n",
    "filename = 'LunarLander-alpha000025-beta00025-400-300.png'\n",
    "plotLearning(score_history, filename, window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "! mkdir tmp/ddpg/Actor_ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1662631021365 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
